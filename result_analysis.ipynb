{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "from PIL import Image\n",
    "\n",
    "import utils.registration as registration\n",
    "import utils.fread as fread\n",
    "import utils.FCGF as FCGF\n",
    "import utils.pointcloud as pointcloud\n",
    "import utils.transform as transform\n",
    "import utils.grid_search as grid_search\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(config: Config):\n",
    "    if not os.path.exists(config.get_output_file(f\"{config.get_file_name()}.npz\")):\n",
    "        print(\"Unable to find trajectory data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\")):\n",
    "        print(\"Unable to find groundtruth data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(config.get_output_file(f\"{config.get_file_name()}__overlap.csv\")):\n",
    "        print(\"Overlap already calculated. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    groundtruth_data = np.load(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\"))\n",
    "    estimated_data = np.load(config.get_output_file(f\"{config.get_file_name()}.npz\"))\n",
    "    \n",
    "    feature_dir = config.get_feature_dir()\n",
    "    sequence_ts = fread.get_timstamps(feature_dir, ext=\".secondary.npz\")\n",
    "    sequence_ts = fread.sample_timestamps(sequence_ts, config.target_fps)\n",
    "    \n",
    "    target_ts = estimated_data[\"sequence_ts\"]\n",
    "\n",
    "    target_inds = [np.argwhere(sequence_ts == target_ts[i])[0][0] for i in range(len(target_ts))]\n",
    "    \n",
    "    local_t = groundtruth_data[\"local_t\"][target_inds]\n",
    "\n",
    "    sequence_ts = sequence_ts[target_inds]\n",
    "    num_frames = len(sequence_ts)\n",
    "\n",
    "    std_values = []\n",
    "\n",
    "    for t in tqdm.trange(len(sequence_ts)):\n",
    "        depth_img_file = os.path.join(config.get_sequence_dir(), f\"frame-{sequence_ts[t]}.depth.png\")\n",
    "        std_values.append(registration.calc_std(depth_img_file, 4000))\n",
    "        \n",
    "    std_values = np.array(std_values)\n",
    "\n",
    "    local_pcds = []\n",
    "\n",
    "    for t in tqdm.trange(num_frames):\n",
    "        feature_file = os.path.join(config.get_feature_dir(), f\"{sequence_ts[t]}.secondary.npz\")\n",
    "        pcd = FCGF.get_features(feature_file, config.voxel_size, pcd_only=True)\n",
    "        local_pcds.append(pcd)\n",
    "        \n",
    "\n",
    "    device_0_ts = fread.get_timstamps_from_images(os.path.join(config.get_global_dir(), \"device-0\"), ext=\".depth.png\")\n",
    "    device_1_ts = fread.get_timstamps_from_images(os.path.join(config.get_global_dir(), \"device-1\"), ext=\".depth.png\")\n",
    "    device_2_ts = fread.get_timstamps_from_images(os.path.join(config.get_global_dir(), \"device-2\"), ext=\".depth.png\")\n",
    "\n",
    "    y = [[], [], []]\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        y[0].append(nearest(device_0_ts, sequence_ts[i]))\n",
    "        y[1].append(nearest(device_1_ts, sequence_ts[i]))\n",
    "        y[2].append(nearest(device_2_ts, sequence_ts[i]))\n",
    "        \n",
    "        y[0][i] = np.abs(y[0][i] - sequence_ts[i]) * 1e-6\n",
    "        y[1][i] = np.abs(y[1][i] - sequence_ts[i]) * 1e-6\n",
    "        y[2][i] = np.abs(y[2][i] - sequence_ts[i]) * 1e-6\n",
    "        \n",
    "    print(\"-- Finding optimal global positions for registration\")\n",
    "\n",
    "    global_frame_delays = np.array(y)\n",
    "    global_frame_delays_inds = np.ones(global_frame_delays.shape, dtype=np.int8)\n",
    "\n",
    "    for r, c in np.argwhere(global_frame_delays > 100):\n",
    "        global_frame_delays_inds[r, c] = 0\n",
    "        \n",
    "    global_frame_delays_inds = np.sum(global_frame_delays_inds, axis=0)\n",
    "    global_frame_delays_inds = np.where(global_frame_delays_inds == 3, 1, 0)\n",
    "        \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(global_frame_delays_inds)\n",
    "    plt.xlabel(\"Frame #\")\n",
    "    plt.ylabel(\"Availability of Global Frame\")\n",
    "\n",
    "    plt.savefig(config.get_output_file(f\"{config.get_file_name()}__availability.jpeg\"))\n",
    "    plt.close()\n",
    "\n",
    "    global_pos = [0]\n",
    "    for t in tqdm.trange(num_frames):\n",
    "        if t - global_pos[-1] >= config.target_fps * 0.8: \n",
    "            global_pos.append(t)\n",
    "            continue\n",
    "        \n",
    "        if (np.abs(std_values[t] - std_values[global_pos[-1]]) > config.delta) and (t - global_pos[-1] > config.target_fps * 0.5):\n",
    "            global_pos.append(t)\n",
    "\n",
    "    global_pos = np.array(global_pos)\n",
    "\n",
    "    cutoffs = registration.find_cutoffs(std_values, config.target_fps, config.min_std, config.threshold)\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(std_values)\n",
    "\n",
    "    plt.scatter(global_pos, std_values[global_pos], c=\"r\", marker=\"+\")\n",
    "\n",
    "    for x in cutoffs:\n",
    "        plt.axvline(x - config.cutoff_margin, c=\"g\", linestyle=\"--\")\n",
    "        plt.axvline(x, c=\"b\", linestyle=\"--\")\n",
    "        plt.axvline(x + config.cutoff_margin, c=\"g\", linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(y=config.min_std, color=\"r\", linestyle=\"--\")\n",
    "    plt.ylim(0, 4)\n",
    "    plt.xlim(0, len(std_values))\n",
    "\n",
    "    plt.xlabel(\"Frame #\")\n",
    "    plt.ylabel(\"Std. of Distances to the camera\")\n",
    "\n",
    "    plt.savefig(config.get_output_file(f\"{config.get_file_name()}__std.jpeg\"))\n",
    "    plt.close()\n",
    "\n",
    "    cutoffs = registration.get_cutoff_sequence(std_values, config.target_fps, config.min_std, config.threshold, config.cutoff_margin)\n",
    "    \n",
    "    target = FCGF.get_features(\"data/reference/larc_kitchen_v5.npz\", config.voxel_size, pcd_only=True)\n",
    "\n",
    "    # plt.figure(figsize=(20, 10))\n",
    "    overlap = []\n",
    "\n",
    "    for i in range(len(cutoffs)):\n",
    "\n",
    "        start_t, end_t = cutoffs[i]\n",
    "        global_inds = global_pos[np.logical_and(global_pos >= start_t, global_pos <= end_t)]\n",
    "        \n",
    "        availability = np.sum(global_frame_delays_inds[start_t:end_t]) / (end_t - start_t)\n",
    "        \n",
    "        fragment = []\n",
    "\n",
    "        for t in range(start_t, end_t):\n",
    "            local_temp = copy.deepcopy(local_pcds[t])\n",
    "            local_temp.transform(local_t[t])\n",
    "            fragment.append(local_temp)\n",
    "            \n",
    "        fragment = pointcloud.merge_pcds(fragment, 0.03)\n",
    "        fragment.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "\n",
    "        result = open3d.registration.evaluate_registration(fragment, target, 0.05, np.identity(4))\n",
    "        \n",
    "        overlap.append([start_t, end_t, result.fitness, len(global_inds) >= 3, availability])\n",
    "        \n",
    "        # gt_points = np.asarray(target.points)\n",
    "        # gt_x, gt_y = gt_points[:, 0], gt_points[:, 2]\n",
    "        \n",
    "        # fg_points = np.asarray(fragment.points)\n",
    "        # fg_x, fg_y = fg_points[:, 0], fg_points[:, 2]\n",
    "        \n",
    "        # plt.subplot(1, len(cutoffs), i + 1)\n",
    "        # plt.scatter(gt_x, gt_y, s=0.1, c=\"r\")\n",
    "        # plt.scatter(fg_x, fg_y, s=0.1, c=\"b\")\n",
    "        # plt.xlim(-4.5, 4)\n",
    "        # plt.ylim(-5.5, 4.5)\n",
    "        # plt.gca().set_aspect('equal', adjustable='box')\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.title(f\"({start_t}, {end_t}) : {result.fitness:.2f}, {len(global_inds) >= 3}\")\n",
    "\n",
    "    df = pd.DataFrame(overlap, columns=[\"start_t\", \"end_t\", \"overlap\", \"has_enough_global_frames\", \"availability\"])\n",
    "    df.to_csv(config.get_output_file(f\"{config.get_file_name()}__overlap.csv\"), index=False)\n",
    "\n",
    "    # plt.savefig(config.get_output_file(f\"{config.get_file_name()}__overlap.jpeg\"))\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "        sequence_dir=\"data/raw_data\",\n",
    "        feature_dir=\"data/features\",\n",
    "        output_dir=\"data/trajectories/trajectory/IMU_PCD_outlier_removed_0.05\",\n",
    "        experiment=\"exp_12\",\n",
    "        trial=\"trial_1\",\n",
    "        subject=\"subject-1\",\n",
    "        sequence=\"01\",\n",
    "        groundtruth_dir=\"data/trajectories/groundtruth\",\n",
    "    )\n",
    "    \n",
    "config.voxel_size=0.05\n",
    "config.target_fps=20\n",
    "config.min_std=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 01\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 02\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 03\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 04\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 05\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 01\n",
      "Overlap already calculated. Skipping.\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:03<00:00, 116.27it/s]\n",
      "100%|██████████| 417/417 [00:06<00:00, 63.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:00<00:00, 285229.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:03<00:00, 117.55it/s]\n",
      "100%|██████████| 416/416 [00:06<00:00, 63.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:00<00:00, 416526.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:03<00:00, 117.48it/s]\n",
      "100%|██████████| 423/423 [00:06<00:00, 67.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:00<00:00, 424345.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:03<00:00, 117.82it/s]\n",
      "100%|██████████| 411/411 [00:06<00:00, 64.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:00<00:00, 384275.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:03<00:00, 118.97it/s]\n",
      "100%|██████████| 414/414 [00:06<00:00, 66.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:00<00:00, 415615.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:03<00:00, 117.62it/s]\n",
      "100%|██████████| 385/385 [00:05<00:00, 66.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:00<00:00, 384661.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:03<00:00, 119.11it/s]\n",
      "100%|██████████| 416/416 [00:05<00:00, 69.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:00<00:00, 311132.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [00:03<00:00, 118.50it/s]\n",
      "100%|██████████| 413/413 [00:06<00:00, 64.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [00:00<00:00, 301260.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:03<00:00, 117.07it/s]\n",
      "100%|██████████| 414/414 [00:06<00:00, 62.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Finding optimal global positions for registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:00<00:00, 319551.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for trial in os.listdir(os.path.join(config.feature_dir, config.experiment)):\n",
    "    config.trial = trial\n",
    "    for subject in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size))):\n",
    "        config.subject = subject    \n",
    "        for sequence in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size), config.subject)):\n",
    "            config.sequence = sequence\n",
    "            print(f\"Processing: {config.experiment} >> {config.trial} >> {config.subject} >> {config.sequence}\")\n",
    "            calc_overlap(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adjusted(config: Config):\n",
    "    if not os.path.exists(config.get_output_file(f\"{config.get_file_name()}.npz\")):\n",
    "        print(\"Unable to find trajectory data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\")):\n",
    "        print(\"Unable to find groundtruth data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    overlap_file = config.get_output_file(f\"{config.get_file_name()}__overlap.csv\")\n",
    "    overlap_df = pd.read_csv(overlap_file)\n",
    "\n",
    "    groundtruth_data = np.load(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\"))\n",
    "    estimated_data = np.load(config.get_output_file(f\"{config.get_file_name()}.npz\"))\n",
    "    \n",
    "    feature_dir = config.get_feature_dir()\n",
    "    sequence_ts = fread.get_timstamps(feature_dir, ext=\".secondary.npz\")\n",
    "    # sequence_ts = fread.sample_timestamps(sequence_ts, config.target_fps)\n",
    "    \n",
    "    target_ts = estimated_data[\"sequence_ts\"]\n",
    "    \n",
    "    target_inds = [np.argwhere(sequence_ts == target_ts[i])[0][0] for i in range(len(target_ts))]\n",
    "    \n",
    "    groundtruth_t = groundtruth_data[\"local_t\"][target_inds]\n",
    "    estimated_t = estimated_data[\"global_t\"]\n",
    "    \n",
    "    # print(len(groundtruth_t), len(estimated_t), groundtruth_t.sum())\n",
    "    \n",
    "    # sequence_ts = sequence_ts[target_inds]\n",
    "    # num_frames = len(sequence_ts)\n",
    "    \n",
    "    # groundtruth_data = np.load(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\"))\n",
    "    # estimated_data = np.load(config.get_output_file(f\"{config.get_file_name()}.npz\"))\n",
    "\n",
    "    # sequence_ts = fread.get_timstamps(config.get_feature_dir(), ext=\".secondary.npz\")\n",
    "    # sequence_ts = fread.sample_timestamps(sequence_ts, config.target_fps)\n",
    "\n",
    "    # estimated_t = estimated_data[\"global_t\"]\n",
    "    # groundtruth_t = groundtruth_data[\"local_t\"]\n",
    "\n",
    "    translation_error = []\n",
    "    rotation_error = []\n",
    "    num_samples = 0\n",
    "    \n",
    "    for start_t, end_t, overlap, has_enough_global_frames, availability in overlap_df.values:\n",
    "        \n",
    "        if overlap < 0.05 or not has_enough_global_frames or availability < 0.5: continue\n",
    "        \n",
    "        for t in range(int(start_t), int(end_t)):\n",
    "            \n",
    "            if np.sum(estimated_t[t]) == 4: continue\n",
    "            if np.sum(groundtruth_t[t]) == 0: continue\n",
    "            \n",
    "            er, et = transform.calc_error(estimated_t[t], groundtruth_t[t])\n",
    "            translation_error.append(et)\n",
    "            rotation_error.append(er)\n",
    "            num_samples += 1\n",
    "            \n",
    "    if len(translation_error) == 0:\n",
    "        print(\"No valid fragments found!\")\n",
    "        return [np.nan, np.nan]\n",
    "        \n",
    "    print(f\"Translation error: {np.mean(translation_error):.3f} ({np.std(translation_error):.3f})\", end=\"\\t\")\n",
    "    print(f\"Rotation error: {np.mean(rotation_error):.3f} ({np.std(rotation_error):.3f})\", end=\"\\t\")\n",
    "    print(f\"Num samples: {num_samples}\")\n",
    "\n",
    "    return [np.mean(translation_error), np.mean(rotation_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 01\n",
      "Translation error: 2.776 (0.812)\tRotation error: 88.891 (1.011)\tNum samples: 126\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 02\n",
      "Translation error: 0.179 (0.408)\tRotation error: 7.897 (29.247)\tNum samples: 167\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 03\n",
      "Translation error: 0.028 (0.038)\tRotation error: 1.097 (1.483)\tNum samples: 195\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 04\n",
      "Translation error: 0.061 (0.028)\tRotation error: 12.606 (33.575)\tNum samples: 281\n",
      "Processing: exp_12 >> trial_1 >> subject-1 >> 05\n",
      "Translation error: 0.051 (0.010)\tRotation error: 0.464 (0.259)\tNum samples: 160\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 01\n",
      "Translation error: 1.839 (1.162)\tRotation error: 105.821 (27.877)\tNum samples: 173\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 02\n",
      "No valid fragments found!\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 03\n",
      "No valid fragments found!\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 04\n",
      "Translation error: 0.066 (0.014)\tRotation error: 9.429 (24.439)\tNum samples: 285\n",
      "Processing: exp_12 >> trial_2 >> subject-1 >> 05\n",
      "Translation error: 0.041 (0.020)\tRotation error: 1.349 (8.336)\tNum samples: 204\n",
      "Processing: exp_12 >> trial_3 >> subject-1 >> 01\n",
      "Translation error: 0.036 (0.007)\tRotation error: 1.051 (0.807)\tNum samples: 152\n",
      "Processing: exp_12 >> trial_3 >> subject-1 >> 02\n",
      "Translation error: 0.145 (0.016)\tRotation error: 1.963 (1.527)\tNum samples: 151\n",
      "Processing: exp_12 >> trial_3 >> subject-1 >> 03\n",
      "Translation error: 0.023 (0.004)\tRotation error: 0.847 (0.193)\tNum samples: 113\n",
      "Processing: exp_12 >> trial_3 >> subject-1 >> 04\n",
      "Translation error: 0.023 (0.006)\tRotation error: 1.158 (8.442)\tNum samples: 202\n",
      "Processing: exp_12 >> trial_3 >> subject-1 >> 05\n",
      "Translation error: 0.038 (0.010)\tRotation error: 2.748 (15.168)\tNum samples: 179\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for trial in os.listdir(os.path.join(config.feature_dir, config.experiment)):\n",
    "    config.trial = trial\n",
    "    for subject in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size))):\n",
    "        config.subject = subject    \n",
    "        for sequence in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size), config.subject)):\n",
    "            config.sequence = sequence\n",
    "            print(f\"Processing: {config.experiment} >> {config.trial} >> {config.subject} >> {config.sequence}\")\n",
    "            # evaluate_adjusted(config)\n",
    "            data.append([config.trial, config.subject, config.sequence] + evaluate_adjusted(config))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"trial\", \"subject\", \"sequence\", \"translation_error\", \"rotation_error\"])\n",
    "output_path = config.get_output_file(\"results_summary.csv\")\n",
    "\n",
    "df.to_csv(config.get_output_file(f\"evaluation_{config.voxel_size}_{config.target_fps}_adjusted.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adjusted(config: Config):\n",
    "    if not os.path.exists(config.get_output_file(f\"{config.get_file_name()}.npz\")):\n",
    "        print(\"Unable to find trajectory data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\")):\n",
    "        print(\"Unable to find groundtruth data. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    overlap_file = os.path.join(\"data/trajectories/trajectory/overlaps\", f\"{config.get_file_name()}__overlap.csv\")\n",
    "    overlap_df = pd.read_csv(overlap_file)\n",
    "\n",
    "    groundtruth_data = np.load(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\"))\n",
    "    estimated_data = np.load(config.get_output_file(f\"{config.get_file_name()}.npz\"))\n",
    "    \n",
    "    feature_dir = config.get_feature_dir()\n",
    "    sequence_ts = fread.get_timstamps(feature_dir, ext=\".secondary.npz\")\n",
    "    \n",
    "    target_ts = estimated_data[\"sequence_ts\"]\n",
    "    \n",
    "    target_inds = [np.argwhere(sequence_ts == target_ts[i])[0][0] for i in range(len(target_ts))]\n",
    "    \n",
    "    groundtruth_t = groundtruth_data[\"local_t\"][target_inds]\n",
    "    estimated_t = estimated_data[\"global_t\"]\n",
    "    \n",
    "    translation_error = []\n",
    "    rotation_error = []\n",
    "    num_samples = 0\n",
    "    \n",
    "    for start_t, end_t, overlap, has_enough_global_frames, availability in overlap_df.values:\n",
    "        \n",
    "        if overlap < 0.05 or not has_enough_global_frames or availability < 0.5: continue\n",
    "        \n",
    "        for t in range(int(start_t), int(end_t)):\n",
    "            \n",
    "            if np.sum(estimated_t[t]) == 4: continue\n",
    "            if np.sum(groundtruth_t[t]) == 0: continue\n",
    "            \n",
    "            er, et = transform.calc_error(estimated_t[t], groundtruth_t[t])\n",
    "            translation_error.append(et)\n",
    "            rotation_error.append(er)\n",
    "            num_samples += 1\n",
    "            \n",
    "    if len(translation_error) == 0:\n",
    "        print(\"No valid fragments found!\")\n",
    "        return [np.nan, np.nan]\n",
    "        \n",
    "    print(f\"Translation error: {np.mean(translation_error):.3f} ({np.std(translation_error):.3f})\", end=\"\\t\")\n",
    "    print(f\"Rotation error: {np.mean(rotation_error):.3f} ({np.std(rotation_error):.3f})\", end=\"\\t\")\n",
    "    print(f\"Num samples: {num_samples}\")\n",
    "\n",
    "    return [np.mean(translation_error), np.mean(rotation_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "        sequence_dir=\"data/raw_data\",\n",
    "        feature_dir=\"data/features\",\n",
    "        output_dir=\"\",\n",
    "        experiment=\"exp_12\",\n",
    "        trial=\"trial_1\",\n",
    "        subject=\"subject-1\",\n",
    "        sequence=\"01\",\n",
    "        groundtruth_dir=\"data/trajectories/groundtruth\",\n",
    "    )\n",
    "    \n",
    "config.voxel_size=0.03\n",
    "config.target_fps=20\n",
    "config.min_std=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = f\"IMU_PCD_{config.voxel_size}\"\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print(f\"Iteration: {i}\")\n",
    "\n",
    "    config.output_dir=f\"data/trajectories/trajectory/{i}/{method}\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for trial in os.listdir(os.path.join(config.feature_dir, config.experiment)):\n",
    "        config.trial = trial\n",
    "        for subject in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size))):\n",
    "            config.subject = subject    \n",
    "            for sequence in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size), config.subject)):\n",
    "                config.sequence = sequence\n",
    "                print(f\"Processing: {config.experiment} >> {config.trial} >> {config.subject} >> {config.sequence}\")\n",
    "                # evaluate_adjusted(config)\n",
    "                data.append([config.trial, config.subject, config.sequence] + evaluate_adjusted(config))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"trial\", \"subject\", \"sequence\", \"translation_error\", \"rotation_error\"])\n",
    "    output_path = config.get_output_file(\"results_summary.csv\")\n",
    "\n",
    "    df.to_csv(os.path.join(\"results/trajectory_performance/iterations\", f\"summary_{method}_{i}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(config: Config):\n",
    "    overlap_file = config.get_output_file(f\"{config.get_file_name()}__overlap.csv\")\n",
    "    overlap_df = pd.read_csv(overlap_file)\n",
    "\n",
    "    sequence_ts = fread.get_timstamps(config.get_feature_dir(), ext=\".secondary.npz\")\n",
    "    # sequence_ts = fread.sample_timestamps(sequence_ts, config.target_fps)\n",
    "    \n",
    "    groundtruth_data = np.load(os.path.join(config.get_groundtruth_dir(), f\"{config.get_file_name()}.gtpose.npz\"))\n",
    "    estimated_data = np.load(config.get_output_file(f\"{config.get_file_name()}.npz\"))\n",
    "    \n",
    "    target_ts = estimated_data[\"sequence_ts\"]\n",
    "\n",
    "    target_inds = [np.argwhere(sequence_ts == target_ts[i])[0][0] for i in range(len(target_ts))]\n",
    "    \n",
    "    groundtruth_t = groundtruth_data[\"local_t\"][target_inds]\n",
    "    estimated_t = estimated_data[\"global_t\"]\n",
    "    \n",
    "    sequence_ts = sequence_ts[target_inds]\n",
    "\n",
    "    num_frames = len(sequence_ts)\n",
    "\n",
    "    local_pcds = []\n",
    "\n",
    "    for t in tqdm.trange(num_frames):\n",
    "        feature_file = os.path.join(config.get_feature_dir(), f\"{sequence_ts[t]}.secondary.npz\")\n",
    "        pcd = FCGF.get_features(feature_file, config.voxel_size, pcd_only=True)\n",
    "        local_pcds.append(pcd)\n",
    "        \n",
    "    target = FCGF.get_features(\"data/reference/larc_kitchen_v4.npz\", config.voxel_size, pcd_only=True)\n",
    "    target_points = np.asarray(target.points)\n",
    "    tx, ty = target_points[:, 0], target_points[:, 2]\n",
    "        \n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    overlap = []\n",
    "\n",
    "    for i, (start_t, end_t, overlap, has_enough_global_frames, availability) in enumerate(overlap_df.values):\n",
    "        estimated_fragment = []\n",
    "        groundtruth_fragment = []\n",
    "\n",
    "        for t in range(int(start_t), int(end_t)):\n",
    "            if np.sum(estimated_t[t]) == 4: continue\n",
    "            if np.sum(groundtruth_t[t]) == 0: continue\n",
    "            \n",
    "            local_temp = copy.deepcopy(local_pcds[t])\n",
    "            local_temp.transform(estimated_t[t])\n",
    "            estimated_fragment.append(local_temp)\n",
    "            \n",
    "            local_temp = copy.deepcopy(local_pcds[t])\n",
    "            local_temp.transform(groundtruth_t[t])\n",
    "            groundtruth_fragment.append(local_temp)\n",
    "            \n",
    "        estimated_fragment = pointcloud.merge_pcds(estimated_fragment, 0.03)\n",
    "        groundtruth_fragment = pointcloud.merge_pcds(groundtruth_fragment, 0.03)\n",
    "        \n",
    "        estimated_points = np.asarray(estimated_fragment.points)\n",
    "        ex, ey = estimated_points[:, 0], estimated_points[:, 2]\n",
    "        \n",
    "        groundtruth_points = np.asarray(groundtruth_fragment.points)\n",
    "        gx, gy = groundtruth_points[:, 0], groundtruth_points[:, 2]\n",
    "        \n",
    "        plt.subplot(1, len(overlap_df), i + 1)\n",
    "        plt.scatter(tx, ty, s=0.1, c=\"r\")\n",
    "        plt.scatter(ex, ey, s=0.1, c=\"b\")\n",
    "        plt.scatter(gx, gy, s=0.1, c=\"g\")\n",
    "        plt.xlim(-4.5, 4)\n",
    "        plt.ylim(-5.5, 4.5)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"({start_t}, {end_t}) : {overlap:.2f}, {availability:.2f}, {has_enough_global_frames}\")\n",
    "\n",
    "    plt.savefig(config.get_output_file(f\"{config.get_file_name()}__trajectories.jpeg\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:06<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412/412 [00:06<00:00, 65.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [00:05<00:00, 70.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:06<00:00, 65.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_1 >> subject-1 >> 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [00:06<00:00, 68.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:05<00:00, 69.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:01<00:00, 292.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:01<00:00, 326.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:01<00:00, 315.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_2 >> subject-1 >> 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:01<00:00, 325.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:01<00:00, 316.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:01<00:00, 321.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [00:01<00:00, 304.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [00:01<00:00, 332.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: exp_12 >> trial_3 >> subject-1 >> 05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:01<00:00, 314.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for trial in os.listdir(os.path.join(config.feature_dir, config.experiment)):\n",
    "    config.trial = trial\n",
    "    for subject in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size))):\n",
    "        config.subject = subject    \n",
    "        for sequence in os.listdir(os.path.join(config.feature_dir, config.experiment, config.trial, str(config.voxel_size), config.subject)):\n",
    "            config.sequence = sequence\n",
    "            print(f\"Processing: {config.experiment} >> {config.trial} >> {config.subject} >> {config.sequence}\")\n",
    "            plot_trajectories(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"FPFH\", \"IMU_PCD\"]\n",
    "voxel_sizes = [0.03, 0.05]\n",
    "num_iterations = 10\n",
    "\n",
    "result_dir = \"results/trajectory_performance/iterations\"\n",
    "\n",
    "for method in methods:\n",
    "    for voxel_size in voxel_sizes:\n",
    "        results = []\n",
    "\n",
    "        for i in range(1, num_iterations):\n",
    "            result_file = os.path.join(result_dir, f\"summary_{method}_{voxel_size}_{i}.csv\")\n",
    "            results.append(pd.read_csv(result_file))\n",
    "            \n",
    "\n",
    "        result = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        result.dropna(inplace=True)\n",
    "\n",
    "        result = result.groupby([\"trial\", \"subject\", \"sequence\"])\n",
    "\n",
    "        result.mean().to_csv(f\"results/trajectory_performance/summary_{method}_{voxel_size}_mean.csv\")\n",
    "        result.std().to_csv(f\"results/trajectory_performance/summary_{method}_{voxel_size}_std.csv\")\n",
    "        result.min().to_csv(f\"results/trajectory_performance/summary_{method}_{voxel_size}_min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"results/trajectory_performance\"\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    results = []\n",
    "    for voxel_size in voxel_sizes:\n",
    "        result_file = os.path.join(result_dir, f\"summary_{method}_{voxel_size}_min.csv\")\n",
    "        results.append(pd.read_csv(result_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fles = glob.glob(\"results/compression/global_registration/0.03/*.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_count = 0\n",
    "m2_count = 0\n",
    "m3_count = 0\n",
    "m4_count = 0\n",
    "total_count = 0\n",
    "total_samples = 0\n",
    "\n",
    "avg_m1_translation_error = 0\n",
    "avg_m2_translation_error = 0\n",
    "avg_m3_translation_error = 0\n",
    "avg_m4_translation_error = 0\n",
    "\n",
    "avg_m1_rotation_error = 0\n",
    "avg_m2_rotation_error = 0\n",
    "avg_m3_rotation_error = 0\n",
    "avg_m4_rotation_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result_fles)):\n",
    "    result1 = np.load(result_fles[i])\n",
    "    result2 = np.load(result_fles[i].replace(\"0.03\", \"0.05\"))\n",
    "\n",
    "    translation_error1 = result1[\"translation_error\"]\n",
    "    rotation_error1 = result1[\"rotation_error\"]\n",
    "    \n",
    "    translation_error2 = result2[\"translation_error\"]\n",
    "    rotation_error2 = result2[\"rotation_error\"]\n",
    "    \n",
    "    invalid_inds = np.where(translation_error1[0] == -1)[0]\n",
    "\n",
    "    translation_error1 = np.delete(translation_error1, invalid_inds, axis=1)\n",
    "    rotation_error1 = np.delete(rotation_error1, invalid_inds, axis=1)\n",
    "    \n",
    "    translation_error2 = np.delete(translation_error2, invalid_inds, axis=1)\n",
    "    rotation_error2 = np.delete(rotation_error2, invalid_inds, axis=1)\n",
    "\n",
    "    m1 = np.logical_and(translation_error1[0] < 0.2, rotation_error1[0] < 5)\n",
    "    m2 = np.logical_and(translation_error1[1] < 0.2, rotation_error1[1] < 5)\n",
    "    m3 = np.logical_and(translation_error2[0] < 0.2, rotation_error2[0] < 5)\n",
    "    m4 = np.logical_and(translation_error2[1] < 0.2, rotation_error2[1] < 5)\n",
    "    \n",
    "    valid_inds = np.logical_or(m1, np.logical_or(m2, np.logical_or(m3, m4)))\n",
    "    \n",
    "    avg_m1_translation_error += translation_error1[0][valid_inds].sum()\n",
    "    avg_m2_translation_error += translation_error1[1][valid_inds].sum()\n",
    "    avg_m3_translation_error += translation_error2[0][valid_inds].sum()\n",
    "    avg_m4_translation_error += translation_error2[1][valid_inds].sum()\n",
    "    \n",
    "    avg_m1_rotation_error += rotation_error1[0][valid_inds].sum()\n",
    "    avg_m2_rotation_error += rotation_error1[1][valid_inds].sum()\n",
    "    avg_m3_rotation_error += rotation_error2[0][valid_inds].sum()\n",
    "    avg_m4_rotation_error += rotation_error2[1][valid_inds].sum()\n",
    "    \n",
    "    # total_samples += len(translation_error1[0])\n",
    "    \n",
    "    # avg_m1_translation_error += translation_error1[0][m1].sum()\n",
    "    # avg_m2_translation_error += translation_error1[1][m2].sum()\n",
    "    # avg_m3_translation_error += translation_error2[0][m3].sum()\n",
    "    # avg_m4_translation_error += translation_error2[1][m4].sum()\n",
    "    \n",
    "    # avg_m1_rotation_error += rotation_error1[0][m1].sum()\n",
    "    # avg_m2_rotation_error += rotation_error1[1][m2].sum()\n",
    "    # avg_m3_rotation_error += rotation_error2[0][m3].sum()\n",
    "    # avg_m4_rotation_error += rotation_error2[1][m4].sum()\n",
    "\n",
    "    # only this one is correct\n",
    "    # valid_inds = np.logical_or(m1, np.logical_or(m2, np.logical_or(m3, m4)))\n",
    "    \n",
    "    # valid_inds = np.logical_or(m1, m2)\n",
    "    # valid_inds = np.array([1 for _ in range(len(valid_inds))])\n",
    "\n",
    "    total_count += valid_inds.sum()\n",
    "    m1_count += m1[valid_inds].sum()\n",
    "    m2_count += m2[valid_inds].sum()\n",
    "    m3_count += m3[valid_inds].sum()\n",
    "    m4_count += m4[valid_inds].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1: 0.7348484848484849\n",
      "m2: 0.6893939393939394\n",
      "m3: 0.8106060606060606\n",
      "m4: 0.803030303030303\n"
     ]
    }
   ],
   "source": [
    "print(f\"m1: {m1_count / total_count}\")\n",
    "print(f\"m2: {m2_count / total_count}\")\n",
    "print(f\"m3: {m3_count / total_count}\")\n",
    "print(f\"m4: {m4_count / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average m1 translation error: 0.647\n",
      "Average m2 translation error: 0.915\n",
      "Average m3 translation error: 0.491\n",
      "Average m4 translation error: 0.587\n",
      "Average m1 rotation error: 22.277\n",
      "Average m2 rotation error: 33.331\n",
      "Average m3 rotation error: 17.940\n",
      "Average m4 rotation error: 25.195\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average m1 translation error: {avg_m1_translation_error / m1_count:.3f}\")\n",
    "print(f\"Average m2 translation error: {avg_m2_translation_error / m2_count:.3f}\")\n",
    "print(f\"Average m3 translation error: {avg_m3_translation_error / m3_count:.3f}\")\n",
    "print(f\"Average m4 translation error: {avg_m4_translation_error / m4_count:.3f}\")\n",
    "\n",
    "print(f\"Average m1 rotation error: {avg_m1_rotation_error / m1_count:.3f}\")\n",
    "print(f\"Average m2 rotation error: {avg_m2_rotation_error / m2_count:.3f}\")\n",
    "print(f\"Average m3 rotation error: {avg_m3_rotation_error / m3_count:.3f}\")\n",
    "print(f\"Average m4 rotation error: {avg_m4_rotation_error / m4_count:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average m1 translation error: 1.033\n",
      "Average m2 translation error: 1.079\n",
      "Average m3 translation error: 0.933\n",
      "Average m4 translation error: 0.965\n",
      "Average m1 rotation error: 54.166\n",
      "Average m2 rotation error: 56.484\n",
      "Average m3 rotation error: 54.191\n",
      "Average m4 rotation error: 54.796\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average m1 translation error: {avg_m1_translation_error / total_samples:.3f}\")\n",
    "print(f\"Average m2 translation error: {avg_m2_translation_error / total_samples:.3f}\")\n",
    "print(f\"Average m3 translation error: {avg_m3_translation_error / total_samples:.3f}\")\n",
    "print(f\"Average m4 translation error: {avg_m4_translation_error / total_samples:.3f}\")\n",
    "\n",
    "print(f\"Average m1 rotation error: {avg_m1_rotation_error / total_samples:.3f}\")\n",
    "print(f\"Average m2 rotation error: {avg_m2_rotation_error / total_samples:.3f}\")\n",
    "print(f\"Average m3 rotation error: {avg_m3_rotation_error / total_samples:.3f}\")\n",
    "print(f\"Average m4 rotation error: {avg_m4_rotation_error / total_samples:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('lidar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71ec0456078ee969ee04deec14dae5ce507385324b4d068644fb8e515e3f77ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
